{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitution des fichiers \"Users\" et \"EmbeddingMatrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import decomposition\n",
    "\n",
    "class ProductionStartUp():\n",
    "\n",
    "    def __init__(self, pathClicks, pathEmbedMatrix):\n",
    "        self.pathClicks = pathClicks\n",
    "        self.pathEmbedMatrix = pathEmbedMatrix\n",
    "\n",
    "    def getArticleRef(self, sr):\n",
    "        sr['article_ref_id'] = dfClicks[dfClicks.user_id==sr['user_id']].loc[dfClicks.click_timestamp==sr['most_resent_clk'],['click_article_id']].values[0][0]\n",
    "        return sr\n",
    "\n",
    "    def mergeUsersClicks(self):\n",
    "        clicks_path = []\n",
    "        clicks_dir = self.pathClicks# contient tous les fichiers de log\n",
    "\n",
    "        clicks_path = clicks_path + sorted(\n",
    "                [\n",
    "                    os.path.join(clicks_dir, fname)\n",
    "                    for fname in os.listdir(clicks_dir)\n",
    "                    if fname.endswith(\".csv\")\n",
    "                ]\n",
    "            )\n",
    "        print(\"Nombre de fichiers CSV: \", len(clicks_path))\n",
    "\n",
    "        _li = []\n",
    "        for filename in clicks_path:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            _li.append(df)\n",
    "\n",
    "        if not os.path.exists('clicks.csv'):\n",
    "            clicks = pd.concat(_li, axis=0, ignore_index=True)# concaténation de tout les fichiers trouvés\n",
    "            clicks.to_csv('clicks.csv')\n",
    "        else:\n",
    "            # dans ce cas il faut ajouter les nouveaux fichiers aux existants\n",
    "            df = pd.read_csv('clicks.csv', index_col=None, header=0)\n",
    "            _li.append(df)\n",
    "            clicks = pd.concat(_li, axis=0, ignore_index=True)# concaténation de tout les fichiers trouvés\n",
    "            clicks.to_csv('clicks.csv')\n",
    "\n",
    "        # agrégation des dataSets\n",
    "        dfArtMeta = pd.read_csv(os.path.join(self.pathClicks, \"articles_metadata.csv\"))\n",
    "\n",
    "        srClicksUniq = clicks.groupby('user_id')['click_article_id'].agg('nunique')\n",
    "        dfUsers = pd.DataFrame(\n",
    "            {\n",
    "                'user_id': srClicksUniq.index.values\n",
    "                ,'nb_click_article': srClicksUniq.values\n",
    "                ,'most_resent_clk': clicks.groupby('user_id')['click_timestamp'].agg('max')\n",
    "            }\n",
    "        )\n",
    "\n",
    "        dfUsers = dfUsers.apply(self.getArticleRef, axis=1)\n",
    "        dfUsers.to_csv(os.path.join(self.pathClicks, \"Users.csv\"))\n",
    "\n",
    "\n",
    "    def zipEmbeddingMatrix(self):\n",
    "\n",
    "        with open(pathEmbedMatrix,'rb') as f:\n",
    "            ndArtEmbed_pca = pickle.load(f)\n",
    "\n",
    "        print(\"Dimensions dataset avant réduction PCA : \", ndArtEmbed.shape)\n",
    "        pca = decomposition.PCA(n_components=0.99)# nous prenons soins de garder 99% des informations \"utiles\"\n",
    "        ndArtEmbed_pca= pca.fit_transform(ndArtEmbed)\n",
    "        print(\"Dimensions dataset après réduction PCA : \", ndArtEmbed_pca.shape)\n",
    "        \n",
    "        with open(pathEmbedMatrix,'wb') as f:\n",
    "            pickle.dump(ndArtEmbed_pca,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startUp = ProductionStartUp('../Ressources/clicks/', '../Ressources/embedMatrix.pkl')\n",
    "startUp.mergeUsersClicks()\n",
    "startUp.zipEmbeddingMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitution des fichiers de la FunctionApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"productStartUp\")\n",
    "os.makedirs(\"productStartUp/functionApp\")\n",
    "os.makedirs(\"productStartUp/functionApp/HttpTrigger1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing productStartUp/functionApp/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile productStartUp/functionApp/requirements.txt\n",
    "azure-functions==1.11.2\n",
    "certifi==2022.5.18.1\n",
    "joblib==1.1.0\n",
    "numpy==1.22.4\n",
    "pandas==1.4.2\n",
    "python-dateutil==2.8.2\n",
    "pytz==2022.1\n",
    "scikit-learn==1.1.1\n",
    "scipy==1.8.1\n",
    "six==1.16.0\n",
    "sklearn==0.0\n",
    "threadpoolctl==3.1.0\n",
    "wincertstore==0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing productStartUp/functionApp/local.settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile productStartUp/functionApp/local.settings.json\n",
    "{\n",
    "  \"IsEncrypted\": false,\n",
    "  \"Values\": {\n",
    "    \"AzureWebJobsStorage\": \"DefaultEndpointsProtocol=https;AccountName=bookshelfrbe2;AccountKey=8FdjkQDCRNgsisEag1CqgqNGUozCyIhygfVhyqZk5y77TeZ0908T8pG8DUfoYNk3EaVpAE4LuaAT+AStwZhwsw==;EndpointSuffix=core.windows.net\",\n",
    "    \"FUNCTIONS_WORKER_RUNTIME\": \"python\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing productStartUp/functionApp/HttpTrigger1/function.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile productStartUp/functionApp/HttpTrigger1/function.json\n",
    "{\n",
    "  \"scriptFile\": \"__init__.py\",\n",
    "  \"bindings\": [\n",
    "    {\n",
    "      \"authLevel\": \"anonymous\",\n",
    "      \"type\": \"httpTrigger\",\n",
    "      \"direction\": \"in\",\n",
    "      \"name\": \"req\",\n",
    "      \"methods\": [\n",
    "        \"get\",\n",
    "        \"post\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"http\",\n",
    "      \"direction\": \"out\",\n",
    "      \"name\": \"$return\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"blob\",\n",
    "      \"direction\": \"in\",\n",
    "      \"name\": \"users\",\n",
    "      \"path\": \"azure-webjobs-hosts/locks/bookshelfrbe/Users.csv\",\n",
    "      \"connection\": \"AzureWebJobsStorage\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"blob\",\n",
    "      \"direction\": \"in\",\n",
    "      \"name\": \"embedMatrix\",\n",
    "      \"dataType\": \"binary\",\n",
    "      \"path\": \"azure-webjobs-hosts/locks/bookshelfrbe/embedMatrix.pkl\",\n",
    "      \"connection\": \"AzureWebJobsStorage\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing productStartUp/functionApp/HttpTrigger1/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productStartUp/functionApp/HttpTrigger1/__init__.py\n",
    "import azure.functions as func\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "def find_top_n_indices(data, top=5):\n",
    "    indexed = enumerate(data)\n",
    "    sorted_data = sorted(indexed, \n",
    "                         key=itemgetter(1), \n",
    "                         reverse=True) \n",
    "    return [d[0] for d in sorted_data[:top]]\n",
    "\n",
    "def recommendFromArticle(article_id, top):\n",
    "    score = []\n",
    "    for i in range(0, len(ndArtEmbed)):\n",
    "        if(article_id != i):\n",
    "            cos_sim = np.dot(ndArtEmbed[article_id], ndArtEmbed[i])/(np.linalg.norm(ndArtEmbed[article_id])*np.linalg.norm(ndArtEmbed[i]))\n",
    "            score.append(cos_sim)\n",
    "\n",
    "    _best_scores = find_top_n_indices(score, top)\n",
    "            \n",
    "    return _best_scores\n",
    "\n",
    "def main(req: func.HttpRequest,\n",
    "        users: func.InputStream,\n",
    "        embedMatrix: func.InputStream) -> func.HttpResponse:\n",
    "\n",
    "    global ndArtEmbed\n",
    "\n",
    "    # Chargement de la dataFrame utilisateurs\n",
    "    bUsers = users.read()\n",
    "    dfUsers = pd.read_csv(BytesIO(bUsers), index_col=False)\n",
    "\n",
    "    # chargement de la matrice d'embedding\n",
    "    temp_path = tempfile.gettempdir()\n",
    "    file_name = os.path.join(temp_path, \"embedMatrix.pkl\")\n",
    "    with open(file_name, \"w+b\") as local_file:\n",
    "        local_file.write(embedMatrix.read())\n",
    "\n",
    "    with open(local_file.name,'rb') as f:\n",
    "        ndArtEmbed = pickle.load(f)\n",
    "\n",
    "    jsonIdUser = req.get_json()\n",
    "\n",
    "    # récupération de l'id de l'article de référence de l'utilisateur\n",
    "    articleRef = dfUsers.loc[dfUsers.user_id==jsonIdUser['userId'],['article_ref_id']].values[0,0]\n",
    "    lReco = recommendFromArticle(articleRef, 5)\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    return func.HttpResponse(\n",
    "        json.dumps(lReco)\n",
    "        ,headers=headers\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramétrage de l'application BookShelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "  \"API_URL\": \"https://bookshelfrbe.azurewebsites.net/api/httptrigger1\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345c1e894b1c967d800c234d45246726e64000d55d459e8e4745e2d6c66d7430"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
